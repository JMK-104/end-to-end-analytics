{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e849c167-13bf-48eb-8c94-f6cd55ca96e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c31e30c-f3fa-4a44-8a3c-bad48d939645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# API Configuration\n",
    "API_URL = \"https://findwork.dev/api/jobs/\"\n",
    "API_TOKEN = \"18c297ab1b0529b4ca1629a2051d8e8d3716f526\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6f58b0-afcb-4a51-802c-5467121691b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define necessary functions, first for fetching jobs from Findwork API, then for loading them into the bronze schema\n",
    "\n",
    "def fetch_all_jobs():\n",
    "    \"\"\"\n",
    "    Fetch all jobs from the API, handling pagination\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    url = API_URL\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Token {API_TOKEN}'\n",
    "    }\n",
    "    \n",
    "    while url:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_jobs.extend(data['results'])\n",
    "            # Get next page URL\n",
    "            url = data['next']  \n",
    "            print(f\"Fetched {len(all_jobs)} jobs so far...\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            break\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "\n",
    "def load_to_database(jobs_data, table_name):\n",
    "    \"\"\"\n",
    "    Load jobs data into end_to_end analytics SQL table\n",
    "    \"\"\"\n",
    "    # Convert to pandas DataFrame\n",
    "    df_pandas = pd.DataFrame(jobs_data)\n",
    "    \n",
    "    # Convert pandas df to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "    # Specify database to load the data into\n",
    "    full_table_name = f'bronze.{table_name}'\n",
    "    \n",
    "    # Write to table (creates if doesn't exist, and appends data if it does)\n",
    "    df_spark.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "    \n",
    "    print(f\"Successfully loaded {len(jobs_data)} records to table '{full_table_name}'\")\n",
    "    \n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e4c642-1d27-4977-96c5-3ba939055715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching jobs from API...\nFetched 100 jobs so far...\nFetched 200 jobs so far...\nFetched 300 jobs so far...\nFetched 400 jobs so far...\nFetched 500 jobs so far...\nFetched 600 jobs so far...\nFetched 700 jobs so far...\nFetched 800 jobs so far...\nFetched 900 jobs so far...\nFetched 1000 jobs so far...\nFetched 1100 jobs so far...\nFetched 1200 jobs so far...\nFetched 1300 jobs so far...\nFetched 1400 jobs so far...\nFetched 1492 jobs so far...\nLoading data to Database...\nSuccessfully loaded 1492 records to table 'bronze.jobs_table'\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Fetch data from API\n",
    "    print(\"Fetching jobs from API...\")\n",
    "    jobs = fetch_all_jobs()\n",
    "    \n",
    "    # Load to Databricks\n",
    "    print(\"Loading data to Database...\")\n",
    "    df = load_to_database(jobs, \"jobs_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fetch_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}