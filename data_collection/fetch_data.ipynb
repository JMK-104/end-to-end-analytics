{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e849c167-13bf-48eb-8c94-f6cd55ca96e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c31e30c-f3fa-4a44-8a3c-bad48d939645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "# API Configuration\n",
    "API_URL = \"https://findwork.dev/api/jobs/\"\n",
    "API_TOKEN = \"18c297ab1b0529b4ca1629a2051d8e8d3716f526\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6f58b0-afcb-4a51-802c-5467121691b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define necessary functions, first for fetching jobs from Findwork API, then for loading them into the bronze schema, finally for logging load events\n",
    "\n",
    "def fetch_all_jobs():\n",
    "    \"\"\"\n",
    "    Fetch all jobs from the API, handling pagination\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    url = API_URL\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Token {API_TOKEN}'\n",
    "    }\n",
    "    \n",
    "    while url:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_jobs.extend(data['results'])\n",
    "            # Get next page URL\n",
    "            url = data['next']  \n",
    "            print(f\"Fetched {len(all_jobs)} jobs so far...\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            break\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "\n",
    "def load_to_database(jobs_data, table_name):\n",
    "    \"\"\"\n",
    "    Load jobs data into end_to_end analytics SQL table\n",
    "    \"\"\"\n",
    "    # Convert to pandas DataFrame\n",
    "    df_pandas = pd.DataFrame(jobs_data)\n",
    "    \n",
    "    # Convert pandas df to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "    # Specify database to load the data into\n",
    "    full_table_name = f'bronze.{table_name}'\n",
    "    \n",
    "    # Write to table (creates if doesn't exist, and appends data if it does)\n",
    "    df_spark.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "    \n",
    "    print(f\"Successfully loaded {len(jobs_data)} records to table '{full_table_name}'\")\n",
    "    \n",
    "    return df_spark\n",
    "\n",
    "\n",
    "def log_load_event(table_name, record_count):\n",
    "    log_df = spark.createDataFrame([{\n",
    "        \"table_name\": table_name,\n",
    "        \"records_loaded\": record_count,\n",
    "        \"load_timestamp\": datetime.utcnow().isoformat()\n",
    "    }])\n",
    "    log_df.write.mode(\"append\").saveAsTable(\"bronze.load_log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e4c642-1d27-4977-96c5-3ba939055715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Fetch data from API\n",
    "    print(\"Fetching jobs from API...\")\n",
    "    jobs = fetch_all_jobs()\n",
    "    \n",
    "    # Load to Databricks\n",
    "    print(\"Loading data to Database...\")\n",
    "    df = load_to_database(jobs, \"jobs_table\")\n",
    "\n",
    "    # Log load event\n",
    "    print(\"Logging load event...\")\n",
    "    log_load_event(\"jobs_table\", len(jobs))\n",
    "\n",
    "    print(\"Jobs Sucessfully Loaded\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8946372934677274,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fetch_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
