{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c664096d-2fad-4153-a191-fad516d114b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-8205414227427578>, line 3\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    CREATE SCHEMA bronze;\u001B[0m\n",
       "\u001B[0m           ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-8205414227427578-2184805308, line 3)"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-8205414227427578>, line 3\u001B[0;36m\u001B[0m\n\u001B[0;31m    CREATE SCHEMA bronze;\u001B[0m\n\u001B[0m           ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Create bonze schema for raw data, collected directly from Findwork API\n",
    "%sql\n",
    "CREATE SCHEMA bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c31e30c-f3fa-4a44-8a3c-bad48d939645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# API Configuration\n",
    "API_URL = \"https://findwork.dev/api/jobs/\"\n",
    "API_TOKEN = \"18c297ab1b0529b4ca1629a2051d8e8d3716f526\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6f58b0-afcb-4a51-802c-5467121691b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define necessary functions, first for fetching jobs from Findwork API, then for loading them into the bronze schema\n",
    "\n",
    "def fetch_all_jobs():\n",
    "    \"\"\"\n",
    "    Fetch all jobs from the API, handling pagination\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    url = API_URL\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Token {API_TOKEN}'\n",
    "    }\n",
    "    \n",
    "    while url:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_jobs.extend(data['results'])\n",
    "            # Get next page URL\n",
    "            url = data['next']  \n",
    "            print(f\"Fetched {len(all_jobs)} jobs so far...\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            break\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "\n",
    "def load_to_database(jobs_data, table_name):\n",
    "    \"\"\"\n",
    "    Load jobs data into end_to_end analytics SQL table\n",
    "    \"\"\"\n",
    "    # Convert to pandas DataFrame\n",
    "    df_pandas = pd.DataFrame(jobs_data)\n",
    "    \n",
    "    # Convert pandas df to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "    # Specify database to load the data into\n",
    "    full_table_name = f'bronze.{table_name}'\n",
    "    \n",
    "    # Write to table (creates if doesn't exist, and appends data if it does)\n",
    "    df_spark.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "    \n",
    "    print(f\"Successfully loaded {len(jobs_data)} records to table '{full_table_name}'\")\n",
    "    \n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e4c642-1d27-4977-96c5-3ba939055715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Fetch data from API\n",
    "    print(\"Fetching jobs from API...\")\n",
    "    jobs = fetch_all_jobs()\n",
    "    \n",
    "    # Load to Databricks\n",
    "    print(\"Loading data to Database...\")\n",
    "    df = load_to_database(jobs, \"jobs_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fetch_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}