{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62752865-c300-43a2-807d-75b7a5c08502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# City and Country\n",
    "The purpose of this notebook is to parse through location text to retireve city, and country data, which will later be used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a94fcc-b6f5-4ce4-ab4b-7e9a2978c13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create silver-level locations dimension table\n",
    "CREATE OR REPLACE TABLE silver.dim_locations AS\n",
    "SELECT\n",
    "    DENSE_RANK() OVER (ORDER BY location) AS location_key,\n",
    "    location\n",
    "FROM (\n",
    "    SELECT DISTINCT location\n",
    "    FROM silver.jobs_table\n",
    "    WHERE location IS NOT NULL\n",
    ")\n",
    ";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089b00e8-2ae1-4fed-816c-4be38e941747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from geotext import GeoText\n",
    "import pycountry\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load silver.dim_locations\n",
    "silver_df = spark.table(\"silver.dim_locations\").toPandas()\n",
    "\n",
    "# Drop existing gold.dim_locations if it already exists, then create a new one\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold.dim_locations\")\n",
    "gold_df.write.saveAsTable(\"gold.dim_locations\")\n",
    "\n",
    "# Helper Functo to extract city and country from the location string\n",
    "def extract_city_country(location):\n",
    "    places = GeoText(location)\n",
    "    city = places.cities[0] if places.cities else None\n",
    "    country = places.countries[0] if places.countries else None\n",
    "    if country:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(country).name\n",
    "        except LookupError:\n",
    "            pass\n",
    "    return pd.Series([city, country])\n",
    "\n",
    "silver_df[['city', 'country']] = silver_df['location'].apply(extract_city_country)\n",
    "\n",
    "# Fill in blanks\n",
    "silver_df['city'] = silver_df['city'].fillna('Unknown')\n",
    "silver_df['country'] = silver_df['country'].fillna('Unknown')\n",
    "\n",
    "# Convert back to Spark \n",
    "gold_df = spark.createDataFrame(silver_df)\n",
    "\n",
    "gold_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"gold.dim_locations\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7629335118647529,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "city_and_country.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
