# SQL Data Analytics Project  
**End-to-End Data Pipeline & Analytics with the Findwork API**

This project demonstrates a complete **data analytics pipeline** â€” from automated data collection to analytical insights â€” built with **Python, SQL, and Databricks**.  
It highlights how modern data engineering practices can be applied to real-world job market data to extract meaningful business insights.

---

## ğŸ“ Project Overview

| Phase | Description |
|-------|--------------|
| **Phase 1 â€” Data Collection** | Fetched live job postings via the [Findwork API](https://findwork.dev) and ingested them into the Databricks environment. |
| **Phase 2 â€” Data Processing** | Transformed data through a **Bronzeâ€“Silverâ€“Gold** layered architecture for data cleaning, enrichment, and modeling. |
| **Phase 3 â€” Data Analysis** | Conducted SQL-based exploratory and business analysis to uncover trends and insights. 

---

## âš™ï¸ Tech Stack

- **Python 3.9+** â€” for API integration and preprocessing  
- **Databricks SQL Warehouse** â€” for query execution and data warehousing  
- **Apache Spark (PySpark)** â€” for scalable transformations  
- **SQL** â€” for analysis and data modeling  
- **GeoText + FuzzyWuzzy** â€” for location extraction and cleaning  
- **YAML** â€” for process configuration and automation  

---

## ğŸ§© Repository Structure

```text
end-to-end-analytics/
â”‚
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data_collection/
â”‚ â”œâ”€â”€ fetch_data.ipynb
â”‚ â””â”€â”€ jobs_api_readme.md
â”‚
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ city_and_country.ipynb
â”‚ â”œâ”€â”€ exploratory_data_analysis.sql
â”‚ â”œâ”€â”€ gold_ddl.sql
â”‚ â”œâ”€â”€ init.ipynb
â”‚ â”œâ”€â”€ silver_transformations.sql
â”‚ â”‚
â”‚ â””â”€â”€ sql_analysis/
â”‚ â”œâ”€â”€ company_level_insights.sql
â”‚ â”œâ”€â”€ geographic_insights.sql
â”‚ â”œâ”€â”€ job_market_overview.sql
â”‚ â””â”€â”€ temporal_trends.sql
â”‚
â””â”€â”€ etl/
â””â”€â”€ data_collection_and_transformation.yaml
```

---

## ğŸ” Data Pipeline Overview

### ğŸ¥‰ **Bronze Layer**
- Raw job posting data fetched via Findwork API (`fetch_data.ipynb`)
- Data automatically refreshed **once per day** using a scheduled Databricks workflow

### ğŸ¥ˆ **Silver Layer**
- Cleansed and standardized the bronze data
- Handled missing fields, normalized column names, and prepared data for analytics

### ğŸ¥‡ **Gold Layer**
- Enriched data with derived attributes such as **city** and **country** (via NLP/GeoText)
- Created analytical models:
  - `gold.fact_job_postings`
  - `gold.dim_locations`
  - `gold.dim_companies`

---

## ğŸ“Š Analytical Questions

Some of the key analytical questions answered include:

1. Which companies are hiring the most across different regions?  
2. Are there countries or cities where hiring activity is growing or declining?  
3. What roles are most frequently advertised globally?  
4. How has job demand evolved over time by employment type?  
5. Which regions show the highest remote job opportunities?  
6. What are the top employers for tech vs non-tech roles?  
7. How does job posting activity vary seasonally?  
8. What are the top hiring cities by job category?  
9. How diverse is the global job market across industries?  
10. Which companies consistently post over multiple months?

*(See `sql/sql_analysis/` for all analytical queries.)*

---

## ğŸš€ Automation & Orchestration

- **Daily refresh** of job data via Databricks job scheduler  
- **ETL configuration** managed in `etl/data_collection_and_transformation.yaml`  
- **Automated Gold Layer build** integrates NLP-based country detection before model creation  

---
